{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Passing with Half Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "dev = torch.device('cuda')\n",
    "g = dgl.rand_graph(30, 100).to(dev)  # Create a graph on GPU w/ 30 nodes and 100 edges.\n",
    "g.ndata['h'] = torch.rand(30, 16).to(dev).half()  # Create fp16 node features.\n",
    "g.edata['w'] = torch.rand(100, 1).to(dev).half()  # Create fp16 edge features.\n",
    "# Use DGL's built-in functions for message passing on fp16 features.\n",
    "g.update_all(fn.u_mul_e('h', 'w', 'm'), fn.sum('m', 'x'))\n",
    "g.ndata['x'].dtype\n",
    "g.apply_edges(fn.u_dot_v('h', 'x', 'hx'))\n",
    "g.edata['hx'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UDFs for message passing on fp16 features.\n",
    "def message(edges):\n",
    "    return {'m': edges.src['h'] * edges.data['w']}\n",
    "def reduce(nodes):\n",
    "    return {'y': torch.sum(nodes.mailbox['m'], 1)}\n",
    "def dot(edges):\n",
    "    return {'hy': (edges.src['h'] * edges.dst['y']).sum(-1, keepdims=True)}\n",
    "g.update_all(message, reduce)\n",
    "g.ndata['y'].dtype\n",
    "g.apply_edges(dot)\n",
    "g.edata['hy'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-to-End Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "def forward(device_type, g, feat, label, mask, model, amp_dtype):\n",
    "    amp_enabled = amp_dtype in (torch.float16, torch.bfloat16)\n",
    "    with autocast(device_type, enabled=amp_enabled, dtype=amp_dtype):\n",
    "        logit = model(g, feat)\n",
    "        loss = F.cross_entropy(logit[mask], label[mask])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def backward(scaler, loss, optimizer):\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.data import RedditDataset\n",
    "from dgl.nn import GATConv\n",
    "from dgl.transforms import AddSelfLoop\n",
    "\n",
    "amp_dtype = torch.bfloat16 # or torch.float16\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GATConv(in_feats, n_hidden, heads[0], activation=F.elu))\n",
    "        self.layers.append(GATConv(n_hidden * heads[0], n_hidden, heads[1], activation=F.elu))\n",
    "        self.layers.append(GATConv(n_hidden * heads[1], n_classes, heads[2], activation=F.elu))\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            h = layer(g, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = h.flatten(1)\n",
    "            else:\n",
    "                h = h.mean(1)\n",
    "        return h\n",
    "\n",
    "# Data loading\n",
    "transform = AddSelfLoop()\n",
    "data = RedditDataset(transform)\n",
    "device_type = 'cuda' # or 'cpu'\n",
    "dev = torch.device(device_type)\n",
    "\n",
    "g = data[0]\n",
    "g = g.int().to(dev)\n",
    "train_mask = g.ndata['train_mask']\n",
    "feat = g.ndata['feat']\n",
    "label = g.ndata['label']\n",
    "\n",
    "in_feats = feat.shape[1]\n",
    "n_hidden = 256\n",
    "n_classes = data.num_classes\n",
    "heads = [1, 1, 1]\n",
    "model = GAT(in_feats, n_hidden, n_classes, heads)\n",
    "model = model.to(dev)\n",
    "model.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = forward(device_type, g, feat, label, train_mask, model, amp_dtype)\n",
    "\n",
    "    if amp_dtype == torch.float16:\n",
    "        # Backprop w/ gradient scaling\n",
    "        backward(scaler, loss, optimizer)\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch {} | Loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFloat16 CPU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import CiteseerGraphDataset\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.transforms import AddSelfLoop\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # two-layer GCN\n",
    "        self.layers.append(\n",
    "            GraphConv(in_size, hid_size, activation=F.relu)\n",
    "        )\n",
    "        self.layers.append(GraphConv(hid_size, out_size))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# Data loading\n",
    "transform = AddSelfLoop()\n",
    "data = CiteseerGraphDataset(transform=transform)\n",
    "\n",
    "g = data[0]\n",
    "g = g.int()\n",
    "train_mask = g.ndata['train_mask']\n",
    "feat = g.ndata['feat']\n",
    "label = g.ndata['label']\n",
    "\n",
    "in_size = feat.shape[1]\n",
    "hid_size = 16\n",
    "out_size = data.num_classes\n",
    "model = GCN(in_size, hid_size, out_size)\n",
    "\n",
    "# Convert model and graph to bfloat16\n",
    "g = dgl.to_bfloat16(g)\n",
    "feat = feat.to(dtype=torch.bfloat16)\n",
    "model = model.to(dtype=torch.bfloat16)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(g, feat)\n",
    "    loss = loss_fcn(logits[train_mask], label[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {} | Loss {}'.format(epoch, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GNN for Node Classification with Neighborhood Sampling\n",
    "#### Define a neighborhood sampler ad data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes, output_nodes, blocks = next(iter(dataloader))\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt your model for minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        x = F.relu(self.conv1(g, x))\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = F.relu(self.conv1(blocks[0], x))\n",
    "        x = F.relu(self.conv2(blocks[1], x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticTwoLayerGCN(in_features, hidden_features, out_features)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, output_nodes, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    output_labels = blocks[-1].dstdata['label']\n",
    "    output_predictions = model(blocks, input_features)\n",
    "    loss = compute_loss(output_labels, output_predictions)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For heterogeneous graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerRGCN(nn.Module):\n",
    "    def __init__(self, in_feat, hidden_feat, out_feat, rel_names):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, hidden_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(hidden_feat, out_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = self.conv1(blocks[0], x)\n",
    "        x = self.conv2(blocks[1], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_nid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StochasticTwoLayerRGCN(in_features, hidden_features, out_features, etypes)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, output_nodes, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    input_features = blocks[0].srcdata     # returns a dict\n",
    "    output_labels = blocks[-1].dstdata     # returns a dict\n",
    "    output_predictions = model(blocks, input_features)\n",
    "    loss = compute_loss(output_labels, output_predictions)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GNN for Edge Classification with Neoghborhood Sampling\n",
    "#### Define a neighborhood sampler and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing edges in the minibatch from the original graph for neighbor sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = g.num_edges()\n",
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, exclude='reverse_id', reverse_eids=torch.cat([\n",
    "        torch.arange(n_edges // 2, n_edges), torch.arange(0, n_edges // 2)]))\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt your model for minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = F.relu(self.conv1(blocks[0], x))\n",
    "        x = F.relu(self.conv2(blocks[1], x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, num_classes, in_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(2 * in_features, num_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        data = torch.cat([edges.src['x'], edges.dst['x']], 1)\n",
    "        return {'score': self.W(data)}\n",
    "\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata['x'] = x\n",
    "            edge_subgraph.apply_edges(self.apply_edges)\n",
    "            return edge_subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = StochasticTwoLayerGCN(\n",
    "            in_features, hidden_features, out_features)\n",
    "        self.predictor = ScorePredictor(num_classes, out_features)\n",
    "\n",
    "    def forward(self, edge_subgraph, blocks, x):\n",
    "        x = self.gcn(blocks, x)\n",
    "        return self.predictor(edge_subgraph, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_features, hidden_features, out_features, num_classes)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, edge_subgraph, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    edge_subgraph = edge_subgraph.to(torch.device('cuda'))\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    edge_labels = edge_subgraph.edata['labels']\n",
    "    edge_predictions = model(edge_subgraph, blocks, input_features)\n",
    "    loss = compute_loss(edge_labels, edge_predictions)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Heterogeneous graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerRGCN(nn.Module):\n",
    "    def __init__(self, in_feat, hidden_feat, out_feat, rel_names):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, hidden_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(hidden_feat, out_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = self.conv1(blocks[0], x)\n",
    "        x = self.conv2(blocks[1], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def __init__(self, num_classes, in_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(2 * in_features, num_classes)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        data = torch.cat([edges.src['x'], edges.dst['x']], 1)\n",
    "        return {'score': self.W(data)}\n",
    "\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata['x'] = x\n",
    "            for etype in edge_subgraph.canonical_etypes:\n",
    "                edge_subgraph.apply_edges(self.apply_edges, etype=etype)\n",
    "            return edge_subgraph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes,\n",
    "                 etypes):\n",
    "        super().__init__()\n",
    "        self.rgcn = StochasticTwoLayerRGCN(\n",
    "            in_features, hidden_features, out_features, etypes)\n",
    "        self.pred = ScorePredictor(num_classes, out_features)\n",
    "\n",
    "    def forward(self, edge_subgraph, blocks, x):\n",
    "        x = self.rgcn(blocks, x)\n",
    "        return self.pred(edge_subgraph, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "sampler = dgl.dataloading.as_edge_prediction_sampler(sampler)\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, exclude='reverse_types',\n",
    "    reverse_etypes={'follow': 'followed by', 'followed by': 'follow',\n",
    "                    'purchase': 'purchased by', 'purchased by': 'purchase'})\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_features, hidden_features, out_features, num_classes, etypes)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, edge_subgraph, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    edge_subgraph = edge_subgraph.to(torch.device('cuda'))\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    edge_labels = edge_subgraph.edata['labels']\n",
    "    edge_predictions = model(edge_subgraph, blocks, input_features)\n",
    "    loss = compute_loss(edge_labels, edge_predictions)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GNN for Link Predcition with Neighborhood Sampling\n",
    "#### Define a neighborhood sampler and data loader with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, negative_sampler=dgl.dataloading.negative_sampler.Uniform(5))\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_seeds, sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        # caches the probability distribution\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        dst = self.weights.multinomial(len(src), replacement=True)\n",
    "        return src, dst\n",
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, negative_sampler=NegativeSampler(g, 5))\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_seeds, sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt your model for minibatch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = F.relu(self.conv1(blocks[0], x))\n",
    "        x = F.relu(self.conv2(blocks[1], x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata['x'] = x\n",
    "            edge_subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'))\n",
    "            return edge_subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.gcn = StochasticTwoLayerGCN(\n",
    "            in_features, hidden_features, out_features)\n",
    "\n",
    "    def forward(self, positive_graph, negative_graph, blocks, x):\n",
    "        x = self.gcn(blocks, x)\n",
    "        pos_score = self.predictor(positive_graph, x)\n",
    "        neg_score = self.predictor(negative_graph, x)\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    # an example hinge loss\n",
    "    n = pos_score.shape[0]\n",
    "    return (neg_score.view(n, -1) - pos_score.view(n, -1) + 1).clamp(min=0).mean()\n",
    "\n",
    "model = Model(in_features, hidden_features, out_features)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, positive_graph, negative_graph, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    positive_graph = positive_graph.to(torch.device('cuda'))\n",
    "    negative_graph = negative_graph.to(torch.device('cuda'))\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    pos_score, neg_score = model(positive_graph, negative_graph, blocks, input_features)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For heterogeneous graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerRGCN(nn.Module):\n",
    "    def __init__(self, in_feat, hidden_feat, out_feat, rel_names):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(in_feat, hidden_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "                rel : dglnn.GraphConv(hidden_feat, out_feat, norm='right')\n",
    "                for rel in rel_names\n",
    "            })\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x = self.conv1(blocks[0], x)\n",
    "        x = self.conv2(blocks[1], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata['x'] = x\n",
    "            for etype in edge_subgraph.canonical_etypes:\n",
    "                edge_subgraph.apply_edges(\n",
    "                    dgl.function.u_dot_v('x', 'x', 'score'), etype=etype)\n",
    "            return edge_subgraph.edata['score']\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_classes,\n",
    "                 etypes):\n",
    "        super().__init__()\n",
    "        self.rgcn = StochasticTwoLayerRGCN(\n",
    "            in_features, hidden_features, out_features, etypes)\n",
    "        self.pred = ScorePredictor()\n",
    "\n",
    "    def forward(self, positive_graph, negative_graph, blocks, x):\n",
    "        x = self.rgcn(blocks, x)\n",
    "        pos_score = self.pred(positive_graph, x)\n",
    "        neg_score = self.pred(negative_graph, x)\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, negative_sampler=dgl.dataloading.negative_sampler.Uniform(5))\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        # caches the probability distribution\n",
    "        self.weights = {\n",
    "            etype: g.in_degrees(etype=etype).float() ** 0.75\n",
    "            for etype in g.canonical_etypes}\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, g, eids_dict):\n",
    "        result_dict = {}\n",
    "        for etype, eids in eids_dict.items():\n",
    "            src, _ = g.find_edges(eids, etype=etype)\n",
    "            src = src.repeat_interleave(self.k)\n",
    "            dst = self.weights[etype].multinomial(len(src), replacement=True)\n",
    "            result_dict[etype] = (src, dst)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eid_dict = {\n",
    "    etype: g.edges(etype=etype, form='eid')\n",
    "    for etype in g.canonical_etypes}\n",
    "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "    sampler, negative_sampler=NegativeSampler(g, 5))\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    g, train_eid_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(in_features, hidden_features, out_features, num_classes, etypes)\n",
    "model = model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for input_nodes, positive_graph, negative_graph, blocks in dataloader:\n",
    "    blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "    positive_graph = positive_graph.to(torch.device('cuda'))\n",
    "    negative_graph = negative_graph.to(torch.device('cuda'))\n",
    "    input_features = blocks[0].srcdata['features']\n",
    "    pos_score, neg_score = model(positive_graph, negative_graph, blocks, input_features)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Custom Graph Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, g, indices):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(dgl.dataloading.Sampler):\n",
    "    def __init__(self, fanouts : list[int]):\n",
    "        super().__init__()\n",
    "        self.fanouts = fanouts\n",
    "\n",
    "    def sample(self, g, seed_nodes):\n",
    "        output_nodes = seed_nodes\n",
    "        subgs = []\n",
    "        for fanout in reversed(self.fanouts):\n",
    "            # Sample a fixed number of neighbors of the current seed nodes.\n",
    "            sg = g.sample_neighbors(seed_nodes, fanout)\n",
    "            # Convert this subgraph to a message flow graph.\n",
    "            sg = dgl.to_block(sg, seed_nodes)\n",
    "            seed_nodes = sg.srcdata[NID]\n",
    "            subgs.insert(0, sg)\n",
    "            input_nodes = seed_nodes\n",
    "        return input_nodes, output_nodes, subgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ...  # the graph to be sampled from\n",
    "train_nids = ...  # an 1-D tensor of training node IDs\n",
    "sampler = NeighborSampler([10, 15])  # create a sampler\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    graph,\n",
    "    train_nids,\n",
    "    sampler,\n",
    "    batch_size=32,    # batch_size decides how many IDs are passed to sampler at once\n",
    "    ...               # other arguments\n",
    ")\n",
    "for i, mini_batch in enumerate(dataloader):\n",
    "    # unpack the mini batch\n",
    "    input_nodes, output_nodes, subgs = mini_batch\n",
    "    train(input_nodes, output_nodes, subgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler for Heterogeenous Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = dgl.heterograph({\n",
    "    ('user', 'like', 'movie') : ...,\n",
    "    ('user', 'follow', 'user') : ...,\n",
    "    ('movie', 'liked-by', 'user') : ...,\n",
    "})\n",
    "train_nids = {'user' : ..., 'movie' : ...}  # training IDs of 'user' and 'movie' nodes\n",
    "sampler = NeighborSampler([10, 15])  # create a sampler\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    hg,\n",
    "    train_nids,\n",
    "    sampler,\n",
    "    batch_size=32,    # batch_size decides how many IDs are passed to sampler at once\n",
    "    ...               # other arguments\n",
    ")\n",
    "for i, mini_batch in enumerate(dataloader):\n",
    "    # unpack the mini batch\n",
    "    # input_nodes and output_nodes are dictionary while subgs are a list of\n",
    "    # heterogeneous graphs\n",
    "    input_nodes, output_nodes, subgs = mini_batch\n",
    "    train(input_nodes, output_nodes, subgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude Edges During Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(Sampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__()\n",
    "        self.fanouts = fanouts\n",
    "\n",
    "    # NOTE: There is an additional third argument. For homogeneous graphs,\n",
    "    #   it is an 1-D tensor of integer IDs. For heterogeneous graphs, it\n",
    "    #   is a dictionary of ID tensors. We usually set its default value to be None.\n",
    "    def sample(self, g, seed_nodes, exclude_eids=None):\n",
    "        output_nodes = seed_nodes\n",
    "        subgs = []\n",
    "        for fanout in reversed(self.fanouts):\n",
    "            # Sample a fixed number of neighbors of the current seed nodes.\n",
    "            sg = g.sample_neighbors(seed_nodes, fanout, exclude_edges=exclude_eids)\n",
    "            # Convert this subgraph to a message flow graph.\n",
    "            sg = dgl.to_block(sg, seed_nodes)\n",
    "            seed_nodes = sg.srcdata[NID]\n",
    "            subgs.insert(0, sg)\n",
    "            input_nodes = seed_nodes\n",
    "        return input_nodes, output_nodes, subgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Custom GNN Module for Mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_feats * 2, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
    "            return self.W(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphConv(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_feats * 2, out_feats)\n",
    "\n",
    "    # h is now a pair of feature tensors for input and output nodes, instead of\n",
    "    # a single feature tensor.\n",
    "    # def forward(self, g, h):\n",
    "    def forward(self, block, h):\n",
    "        # with g.local_scope():\n",
    "        with block.local_scope():\n",
    "            # g.ndata['h'] = h\n",
    "            h_src = h\n",
    "            h_dst = h[:block.number_of_dst_nodes()]\n",
    "            block.srcdata['h'] = h_src\n",
    "            block.dstdata['h'] = h_dst\n",
    "\n",
    "            # g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
    "            block.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
    "\n",
    "            # return self.W(torch.cat([g.ndata['h'], g.ndata['h_neigh']], 1))\n",
    "            return self.W(torch.cat(\n",
    "                [block.dstdata['h'], block.dstdata['h_neigh']], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heterogeneous Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHeteroGraphConv(nn.Module):\n",
    "    def __init__(self, g, in_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.Ws = nn.ModuleDict()\n",
    "        for etype in g.canonical_etypes:\n",
    "            utype, _, vtype = etype\n",
    "            self.Ws[etype] = nn.Linear(in_feats[utype], out_feats[vtype])\n",
    "        for ntype in g.ntypes:\n",
    "            self.Vs[ntype] = nn.Linear(in_feats[ntype], out_feats[ntype])\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            for ntype in g.ntypes:\n",
    "                g.nodes[ntype].data['h_dst'] = self.Vs[ntype](h[ntype])\n",
    "                g.nodes[ntype].data['h_src'] = h[ntype]\n",
    "            for etype in g.canonical_etypes:\n",
    "                utype, _, vtype = etype\n",
    "                g.update_all(\n",
    "                    fn.copy_u('h_src', 'm'), fn.mean('m', 'h_neigh'),\n",
    "                    etype=etype)\n",
    "                g.nodes[vtype].data['h_dst'] = g.nodes[vtype].data['h_dst'] + \\\n",
    "                    self.Ws[etype](g.nodes[vtype].data['h_neigh'])\n",
    "            return {ntype: g.nodes[ntype].data['h_dst'] for ntype in g.ntypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHeteroGraphConv(nn.Module):\n",
    "    def __init__(self, g, in_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.Ws = nn.ModuleDict()\n",
    "        for etype in g.canonical_etypes:\n",
    "            utype, _, vtype = etype\n",
    "            self.Ws[etype] = nn.Linear(in_feats[utype], out_feats[vtype])\n",
    "        for ntype in g.ntypes:\n",
    "            self.Vs[ntype] = nn.Linear(in_feats[ntype], out_feats[ntype])\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            for ntype in g.ntypes:\n",
    "                h_src, h_dst = h[ntype]\n",
    "                g.dstnodes[ntype].data['h_dst'] = self.Vs[ntype](h[ntype])\n",
    "                g.srcnodes[ntype].data['h_src'] = h[ntype]\n",
    "            for etype in g.canonical_etypes:\n",
    "                utype, _, vtype = etype\n",
    "                g.update_all(\n",
    "                    fn.copy_u('h_src', 'm'), fn.mean('m', 'h_neigh'),\n",
    "                    etype=etype)\n",
    "                g.dstnodes[vtype].data['h_dst'] = \\\n",
    "                    g.dstnodes[vtype].data['h_dst'] + \\\n",
    "                    self.Ws[etype](g.dstnodes[vtype].data['h_neigh'])\n",
    "            return {ntype: g.dstnodes[ntype].data['h_dst']\n",
    "                    for ntype in g.ntypes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing modules that work on homogeneous graphs, bipartite graphs, and MFGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "class SAGEConv(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_feats * 2, out_feats)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        if isinstance(h, tuple):\n",
    "            h_src, h_dst = h\n",
    "        elif g.is_block:\n",
    "            h_src = h\n",
    "            h_dst = h[:g.number_of_dst_nodes()]\n",
    "        else:\n",
    "            h_src = h_dst = h\n",
    "\n",
    "        g.srcdata['h'] = h_src\n",
    "        g.dstdata['h'] = h_dst\n",
    "        g.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h_neigh'))\n",
    "        return F.relu(\n",
    "            self.W(torch.cat([g.dstdata['h'], g.dstdata['h_neigh']], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Offline Inference on Large Graphs\n",
    "#### Implementing Offline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticTwoLayerGCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.conv1 = dgl.nn.GraphConv(in_features, hidden_features)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_features, out_features)\n",
    "        self.n_layers = 2\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        x_dst = x[:blocks[0].number_of_dst_nodes()]\n",
    "        x = F.relu(self.conv1(blocks[0], (x, x_dst)))\n",
    "        x_dst = x[:blocks[1].number_of_dst_nodes()]\n",
    "        x = F.relu(self.conv2(blocks[1], (x, x_dst)))\n",
    "        return x\n",
    "\n",
    "    def inference(self, g, x, batch_size, device):\n",
    "        \"\"\"\n",
    "        Offline inference with this module\n",
    "        \"\"\"\n",
    "        # Compute representations layer by layer\n",
    "        for l, layer in enumerate([self.conv1, self.conv2]):\n",
    "            y = torch.zeros(g.num_nodes(),\n",
    "                            self.hidden_features\n",
    "                            if l != self.n_layers - 1\n",
    "                            else self.out_features)\n",
    "            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "            dataloader = dgl.dataloading.NodeDataLoader(\n",
    "                g, torch.arange(g.num_nodes()), sampler,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False)\n",
    "\n",
    "            # Within a layer, iterate over nodes in batches\n",
    "            for input_nodes, output_nodes, blocks in dataloader:\n",
    "                block = blocks[0]\n",
    "\n",
    "                # Copy the features of necessary input nodes to GPU\n",
    "                h = x[input_nodes].to(device)\n",
    "                # Compute output.  Note that this computation is the same\n",
    "                # but only for a single layer.\n",
    "                h_dst = h[:block.number_of_dst_nodes()]\n",
    "                h = F.relu(layer(block, (h, h_dst)))\n",
    "                # Copy to output back to CPU.\n",
    "                y[output_nodes] = h.cpu()\n",
    "\n",
    "            x = y\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling Prefetching with DGL's Builtin Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ...                 # the graph to sample from\n",
    "graph.ndata['feat'] = ...   # node feature\n",
    "graph.ndata['label'] = ...  # node label\n",
    "train_nids = ...  # an 1-D integer tensor of training node IDs\n",
    "# create a sample and specify what data to prefetch\n",
    "sampler = dgl.dataloading.NeighborSampler(\n",
    "    [15, 10, 5], prefetch_node_feats=['feat'], prefetch_labels=['label'])\n",
    "# create a dataloader\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=32,\n",
    "    ...    # other arguments\n",
    ")\n",
    "for mini_batch in dataloader:\n",
    "    # unpack mini batch\n",
    "    input_nodes, output_nodes, subgs = mini_batch\n",
    "    # the following data has been pre-fetched\n",
    "    feat = subgs[0].srcdata['feat']\n",
    "    label = subgs[-1].dstdata['label']\n",
    "    train(subgs, feat, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling Prefetching in Custom Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(dgl.dataloading.Sampler):\n",
    "    def __init__(self,\n",
    "                 fanouts : list[int],\n",
    "                 prefetch_node_feats: list[str] = None,\n",
    "                 prefetch_edge_feats: list[str] = None,\n",
    "                 prefetch_labels: list[str] = None):\n",
    "        super().__init__()\n",
    "        self.fanouts = fanouts\n",
    "        self.prefetch_node_feats = prefetch_node_feats\n",
    "        self.prefetch_edge_feats = prefetch_edge_feats\n",
    "        self.prefetch_labels = prefetch_labels\n",
    "\n",
    "    def sample(self, g, seed_nodes):\n",
    "        output_nodes = seed_nodes\n",
    "        subgs = []\n",
    "        for fanout in reversed(self.fanouts):\n",
    "            # Sample a fixed number of neighbors of the current seed nodes.\n",
    "            sg = g.sample_neighbors(seed_nodes, fanout)\n",
    "            # Convert this subgraph to a message flow graph.\n",
    "            sg = dgl.to_block(sg, seed_nodes)\n",
    "            seed_nodes = sg.srcdata[NID]\n",
    "            subgs.insert(0, sg)\n",
    "         input_nodes = seed_nodes\n",
    "\n",
    "         # handle prefetching\n",
    "         dgl.set_src_lazy_features(subgs[0], self.prefetch_node_feats)\n",
    "         dgl.set_dst_lazy_features(subgs[-1], self.prefetch_labels)\n",
    "         for subg in subgs:\n",
    "             dgl.set_edge_lazy_features(subg, self.prefetch_edge_feats)\n",
    "\n",
    "         return input_nodes, output_nodes, subgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dglemv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.dataloading import NeighborSampler\n",
    "from dgl.distributed import DistGraph, DistDataLoader, node_split\n",
    "import torch as th\n",
    "\n",
    "# initialize distributed contexts\n",
    "dgl.distributed.initialize('ip_config.txt')\n",
    "th.distributed.init_process_group(backend='gloo')\n",
    "# load distributed graph\n",
    "g = DistGraph('graph_name', 'part_config.json')\n",
    "pb = g.get_partition_book()\n",
    "# get training workload, i.e., training node IDs\n",
    "train_nid = node_split(g.ndata['train_mask'], pb, force_even=True)\n",
    "\n",
    "\n",
    "# Create sampler\n",
    "sampler = NeighborSampler(g, [10,25],\n",
    "                          dgl.distributed.sample_neighbors,\n",
    "                          device)\n",
    "\n",
    "dataloader = DistDataLoader(\n",
    "    dataset=train_nid.numpy(),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=sampler.sample_blocks,\n",
    "    shuffle=True,\n",
    "    drop_last=False)\n",
    "\n",
    "# Define model and optimizer\n",
    "model = SAGE(in_feats, num_hidden, n_classes, num_layers, F.relu, dropout)\n",
    "model = th.nn.parallel.DistributedDataParallel(model)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(args.num_epochs):\n",
    "    with model.join():\n",
    "        for step, blocks in enumerate(dataloader):\n",
    "            batch_inputs, batch_labels = load_subtensor(g, blocks[0].srcdata[dgl.NID],\n",
    "                                                        blocks[-1].dstdata[dgl.NID])\n",
    "            batch_pred = model(blocks, batch_inputs)\n",
    "            loss = loss_fcn(batch_pred, batch_labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocesing\n",
    "#### Partitioning API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "g = ...  # create or load a DGLGraph object\n",
    "dgl.distributed.partition_graph(g, 'mygraph', 2, 'data_root_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.distributed.partition_graph(g, 'graph_name', 4, '/tmp/test', balance_ntypes=g.ndata['train_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map, edge_map = dgl.distributed.partition_graph(g, 'graph_name', 4, '/tmp/test',\n",
    "                                                     balance_ntypes=g.ndata['train_mask'],\n",
    "                                                     return_mapping=True)\n",
    "# Let's assume that node_emb is saved from the distributed training.\n",
    "orig_node_emb = th.zeros(node_emb.shape, dtype=node_emb.dtype)\n",
    "orig_node_emb[node_map] = node_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Partitioned Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "# load partition 0\n",
    "part_data = dgl.distributed.load_partition('data_root_dir/graph_name.json', 0)\n",
    "g, nfeat, efeat, partition_book, graph_name, ntypes, etypes = part_data  # unpack\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming APIs\n",
    "#### DistGraph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "g = dgl.distributed.DistGraph('graph_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "g = dgl.distributed.DistGraph('graph_name', part_config='data/graph_name.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accsssing Graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.num_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing node/edge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['train_mask']  # <dgl.distributed.dist_graph.DistTensor at 0x7fec820937b8>\n",
    "g.ndata['train_mask'][0]  # tensor([1], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = dgl.distributed.DistTensor((g.num_nodes(), 10), th.float32, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['feat'] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = g.ndata['feat'][[1, 2, 3]]\n",
    "print(data)\n",
    "g.ndata['feat'][[3, 4, 5]] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed DistEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(shape, dtype):\n",
    "    arr = th.zeros(shape, dtype=dtype)\n",
    "    arr.uniform_(-1, 1)\n",
    "    return arr\n",
    "emb = dgl.distributed.DistEmbedding(g.num_nodes(), 10, init_func=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_optimizer = dgl.distributed.SparseAdagrad([emb], lr=lr1)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr2)\n",
    "feats = emb(nids)\n",
    "loss = model(feats)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "sparse_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_blocks(seeds):\n",
    "    seeds = th.LongTensor(np.asarray(seeds))\n",
    "    blocks = []\n",
    "    for fanout in [10, 25]:\n",
    "        frontier = dgl.distributed.sample_neighbors(g, seeds, fanout, replace=True)\n",
    "        block = dgl.to_block(frontier, seeds)\n",
    "        seeds = block.srcdata[dgl.NID]\n",
    "        blocks.insert(0, block)\n",
    "        return blocks\n",
    "    dataloader = dgl.distributed.DistDataLoader(dataset=train_nid,\n",
    "                                                batch_size=batch_size,\n",
    "                                                collate_fn=sample_blocks,\n",
    "                                                shuffle=True)\n",
    "    for batch in dataloader:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.sampling.MultiLayerNeighborSampler([10, 25])\n",
    "dataloader = dgl.sampling.DistNodeDataLoader(g, train_nid, sampler,\n",
    "                                             batch_size=batch_size, shuffle=True)\n",
    "for batch in dataloader:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nids = dgl.distributed.node_split(g.ndata['train_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Graph Partitioning\n",
    "#### METIS partition algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "# load partition 0\n",
    "part_data = dgl.distributed.load_partition('data_root_dir/graph_name.json', 0)\n",
    "g, nfeat, efeat, partition_book, graph_name, ntypes, etypes = part_data  # unpack\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Graph Under the Hood\n",
    "#### ID Conversion Utilities\n",
    "#### During Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "\n",
    "class IDConverter:\n",
    "    def __init__(self, meta):\n",
    "        # meta is the JSON object loaded from metadata.json\n",
    "        self.node_type = meta['node_type']\n",
    "        self.edge_type = meta['edge_type']\n",
    "        self.ntype2id_map = {ntype : i for i, ntype in enumerate(self.node_type)}\n",
    "        self.etype2id_map = {etype : i for i, etype in enumerate(self.edge_type)}\n",
    "        self.num_nodes = [sum(ns) for ns in meta['num_nodes_per_chunk']]\n",
    "        self.num_edges = [sum(ns) for ns in meta['num_edges_per_chunk']]\n",
    "        self.nid_offset = np.cumsum([0] + self.num_nodes)\n",
    "        self.eid_offset = np.cumsum([0] + self.num_edges)\n",
    "\n",
    "    def ntype2id(self, ntype):\n",
    "        \"\"\"From node type name to node type ID\"\"\"\n",
    "        return self.ntype2id_map[ntype]\n",
    "\n",
    "    def etype2id(self, etype):\n",
    "        \"\"\"From edge type name to edge type ID\"\"\"\n",
    "        return self.etype2id_map[etype]\n",
    "\n",
    "    def id2ntype(self, id):\n",
    "        \"\"\"From node type ID to node type name\"\"\"\n",
    "        return self.node_type[id]\n",
    "\n",
    "    def id2etype(self, id):\n",
    "        \"\"\"From edge type ID to edge type name\"\"\"\n",
    "        return self.edge_type[id]\n",
    "\n",
    "    def nid_het2hom(self, ntype, id):\n",
    "        \"\"\"From heterogeneous node ID to homogeneous node ID\"\"\"\n",
    "        tid = self.ntype2id(ntype)\n",
    "        if id < 0 or id >= self.num_nodes[tid]:\n",
    "            raise ValueError(f'Invalid node ID of type {ntype}. Must be within range [0, {self.num_nodes[tid]})')\n",
    "        return self.nid_offset[tid] + id\n",
    "\n",
    "    def nid_hom2het(self, id):\n",
    "        \"\"\"From heterogeneous node ID to homogeneous node ID\"\"\"\n",
    "        if id < 0 or id >= self.nid_offset[-1]:\n",
    "            raise ValueError(f'Invalid homogeneous node ID. Must be within range [0, self.nid_offset[-1])')\n",
    "        tid = bisect_left(self.nid_offset, id) - 1\n",
    "        # Return a pair (node_type, type_wise_id)\n",
    "        return self.id2ntype(tid), id - self.nid_offset[tid]\n",
    "\n",
    "    def eid_het2hom(self, etype, id):\n",
    "        \"\"\"From heterogeneous edge ID to homogeneous edge ID\"\"\"\n",
    "        tid = self.etype2id(etype)\n",
    "        if id < 0 or id >= self.num_edges[tid]:\n",
    "            raise ValueError(f'Invalid edge ID of type {etype}. Must be within range [0, {self.num_edges[tid]})')\n",
    "        return self.eid_offset[tid] + id\n",
    "\n",
    "    def eid_hom2het(self, id):\n",
    "        \"\"\"From heterogeneous edge ID to homogeneous edge ID\"\"\"\n",
    "        if id < 0 or id >= self.eid_offset[-1]:\n",
    "            raise ValueError(f'Invalid homogeneous edge ID. Must be within range [0, self.eid_offset[-1])')\n",
    "        tid = bisect_left(self.eid_offset, id) - 1\n",
    "        # Return a pair (edge_type, type_wise_id)\n",
    "        return self.id2etype(tid), id - self.eid_offset[tid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Partition Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpb = g.get_partition_book()\n",
    "# We need to map the type-wise node IDs to homogeneous IDs.\n",
    "cur = gpb.map_to_homo_nid(seeds, 'paper')\n",
    "# For a heterogeneous input graph, the returned frontier is stored in\n",
    "# the homogeneous graph format.\n",
    "frontier = dgl.distributed.sample_neighbors(g, cur, fanout, replace=False)\n",
    "block = dgl.to_block(frontier, cur)\n",
    "cur = block.srcdata[dgl.NID]\n",
    "\n",
    "block.edata[dgl.EID] = frontier.edata[dgl.EID]\n",
    "# Map the homogeneous edge Ids to their edge type.\n",
    "block.edata[dgl.ETYPE], block.edata[dgl.EID] = gpb.map_to_per_etype(block.edata[dgl.EID])\n",
    "# Map the homogeneous node Ids to their node types and per-type Ids.\n",
    "block.srcdata[dgl.NTYPE], block.srcdata[dgl.NID] = gpb.map_to_per_ntype(block.srcdata[dgl.NID])\n",
    "block.dstdata[dgl.NTYPE], block.dstdata[dgl.NID] = gpb.map_to_per_ntype(block.dstdata[dgl.NID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access distributed graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "g = dgl.distributed.DistGraph('graph_name', part_config='data/graph_name.json')\n",
    "feat = g.nodes['T0'].data['feat'][[0, 10, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes['T0'].data['feat1'] = dgl.distributed.DistTensor(\n",
    "    (g.num_nodes('T0'), 1), th.float32, 'feat1',\n",
    "    part_policy=g.get_node_partition_policy('T0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dglemv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

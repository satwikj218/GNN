{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGLDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    \"\"\" Template for customizing graph datasets in DGL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL to download the raw dataset\n",
    "    raw_dir : str\n",
    "        Specifying the directory that will store the\n",
    "        downloaded data or the directory that\n",
    "        already stores the input data.\n",
    "        Default: ~/.dgl/\n",
    "    save_dir : str\n",
    "        Directory to save the processed dataset.\n",
    "        Default: the value of `raw_dir`\n",
    "    force_reload : bool\n",
    "        Whether to reload the dataset. Default: False\n",
    "    verbose : bool\n",
    "        Whether to print out progress information\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='dataset_name',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def download(self):\n",
    "        # download raw data to local disk\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # process raw data to graphs, labels, splitting masks\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get one example by index\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of data examples\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        # save processed data to directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        # load processed data from directory `self.save_path`\n",
    "        pass\n",
    "\n",
    "    def has_cache(self):\n",
    "        # check whether there are processed data in `self.save_path`\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl.data.utils import download\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    file_path = os.path.join(self.raw_dir, self.name + '.mat')\n",
    "    # download file\n",
    "    download(self.url, path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import download, check_sha1\n",
    "\n",
    "def download(self):\n",
    "    # path to store the file\n",
    "    # make sure to use the same suffix as the original file name's\n",
    "    gz_file_path = os.path.join(self.raw_dir, self.name + '.csv.gz')\n",
    "    # download file\n",
    "    download(self.url, path=gz_file_path)\n",
    "    # check SHA-1\n",
    "    if not check_sha1(gz_file_path, self._sha1_str):\n",
    "        raise UserWarning('File {} is downloaded but the content hash does not match.'\n",
    "                          'The repo may be outdated or download may be incomplete. '\n",
    "                          'Otherwise you can create an issue for it.'.format(self.name + '.csv.gz'))\n",
    "    # extract file to directory `self.name` under `self.raw_dir`\n",
    "    self._extract_gz(gz_file_path, self.raw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "#### Processing Graph Classification datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLDataset\n",
    "\n",
    "class QM7bDataset(DGLDataset):\n",
    "    _url = 'http://deepchem.io.s3-website-us-west-1.amazonaws.com/' \\\n",
    "           'datasets/qm7b.mat'\n",
    "    _sha1_str = '4102c744bb9d6fd7b40ac67a300e49cd87e28392'\n",
    "\n",
    "    def __init__(self, raw_dir=None, force_reload=False, verbose=False):\n",
    "        super(QM7bDataset, self).__init__(name='qm7b',\n",
    "                                          url=self._url,\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        mat_path = self.raw_path + '.mat'\n",
    "        # process data to a list of graphs and a list of labels\n",
    "        self.graphs, self.label = self._load_graph(mat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def num_tasks(self):\n",
    "    \"\"\"Number of labels for each graph, i.e. number of prediction tasks.\"\"\"\n",
    "    return 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "# load data\n",
    "dataset = QM7bDataset()\n",
    "num_tasks = dataset.num_tasks\n",
    "\n",
    "# create dataloaders\n",
    "dataloader = GraphDataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# training\n",
    "for epoch in range(100):\n",
    "    for g, labels in dataloader:\n",
    "        # your training code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Node Classification datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import DGLBuiltinDataset\n",
    "from dgl.data.utils import _get_dgl_url\n",
    "\n",
    "class CitationGraphDataset(DGLBuiltinDataset):\n",
    "    _urls = {\n",
    "        'cora_v2' : 'dataset/cora_v2.zip',\n",
    "        'citeseer' : 'dataset/citeseer.zip',\n",
    "        'pubmed' : 'dataset/pubmed.zip',\n",
    "    }\n",
    "\n",
    "    def __init__(self, name, raw_dir=None, force_reload=False, verbose=True):\n",
    "        assert name.lower() in ['cora', 'citeseer', 'pubmed']\n",
    "        if name.lower() == 'cora':\n",
    "            name = 'cora_v2'\n",
    "        url = _get_dgl_url(self._urls[name])\n",
    "        super(CitationGraphDataset, self).__init__(name,\n",
    "                                                   url=url,\n",
    "                                                   raw_dir=raw_dir,\n",
    "                                                   force_reload=force_reload,\n",
    "                                                   verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # build graph\n",
    "        g = dgl.graph(graph)\n",
    "        # splitting masks\n",
    "        g.ndata['train_mask'] = train_mask\n",
    "        g.ndata['val_mask'] = val_mask\n",
    "        g.ndata['test_mask'] = test_mask\n",
    "        # node labels\n",
    "        g.ndata['label'] = torch.tensor(labels)\n",
    "        # node features\n",
    "        g.ndata['feat'] = torch.tensor(_preprocess_features(features),\n",
    "                                       dtype=F.data_type_dict['float32'])\n",
    "        self._num_tasks = onehot_labels.shape[1]\n",
    "        self._labels = labels\n",
    "        # reorder graph to obtain better locality.\n",
    "        self._g = dgl.reorder_graph(g)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = CiteseerGraphDataset(raw_dir='')\n",
    "graph = dataset[0]\n",
    "\n",
    "# get split masks\n",
    "train_mask = graph.ndata['train_mask']\n",
    "val_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "\n",
    "# get node features\n",
    "feats = graph.ndata['feat']\n",
    "\n",
    "# get labels\n",
    "labels = graph.ndata['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing dataset for Link Prediction datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for creating Link Prediction datasets\n",
    "class KnowledgeGraphDataset(DGLBuiltinDataset):\n",
    "    def __init__(self, name, reverse=True, raw_dir=None, force_reload=False, verbose=True):\n",
    "        self._name = name\n",
    "        self.reverse = reverse\n",
    "        url = _get_dgl_url('dataset/') + '{}.tgz'.format(name)\n",
    "        super(KnowledgeGraphDataset, self).__init__(name,\n",
    "                                                    url=url,\n",
    "                                                    raw_dir=raw_dir,\n",
    "                                                    force_reload=force_reload,\n",
    "                                                    verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # Skip some processing code\n",
    "        # === data processing skipped ===\n",
    "\n",
    "        # splitting mask\n",
    "        g.edata['train_mask'] = train_mask\n",
    "        g.edata['val_mask'] = val_mask\n",
    "        g.edata['test_mask'] = test_mask\n",
    "        # edge type\n",
    "        g.edata['etype'] = etype\n",
    "        # node type\n",
    "        g.ndata['ntype'] = ntype\n",
    "        self._g = g\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, \"This dataset has only one graph\"\n",
    "        return self._g\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import FB15k237Dataset\n",
    "\n",
    "# load data\n",
    "dataset = FB15k237Dataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "# get training mask\n",
    "train_mask = graph.edata['train_mask']\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "src, dst = graph.edges(train_idx)\n",
    "# get edge types in training set\n",
    "rel = graph.edata['etype'][train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data.utils import makedirs, save_info, load_info\n",
    "\n",
    "def save(self):\n",
    "    # save graphs and labels\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    save_graphs(graph_path, self.graphs, {'labels': self.labels})\n",
    "    # save other information in python dict\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    save_info(info_path, {'num_classes': self.num_classes})\n",
    "\n",
    "def load(self):\n",
    "    # load processed data from directory `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    self.graphs, label_dict = load_graphs(graph_path)\n",
    "    self.labels = label_dict['labels']\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    self.num_classes = load_info(info_path)['num_classes']\n",
    "\n",
    "def has_cache(self):\n",
    "    # check whether there are processed data in `self.save_path`\n",
    "    graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "    info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "    return os.path.exists(graph_path) and os.path.exists(info_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OGB datasets using ogb package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (1.25.2)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from ogb) (1.26.16)\n",
      "Collecting outdated>=0.2.0 (from ogb)\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools>=44 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (68.0.0)\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sjonnal5/miniconda3/envs/dglemv/lib/python3.11/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=4c39e96bfed8e009e3fe139b584e0691f0a6abd2aa1e5d9195612c9e7c6b6ca0\n",
      "  Stored in directory: /home/sjonnal5/.cache/pip/wheels/17/8d/65/9a39917567093c895549811c172be5d2dfb63c7e4b143e05a4\n",
      "Successfully built littleutils\n",
      "Installing collected packages: littleutils, outdated, ogb\n",
      "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Graph Property Prediction datasets in OGB\n",
    "import dgl\n",
    "import torch\n",
    "from ogb.graphproppred import DglGraphPropPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    # batch is a list of tuple (graph, label)\n",
    "    graphs = [e[0] for e in batch]\n",
    "    g = dgl.batch(graphs)\n",
    "    labels = [e[1] for e in batch]\n",
    "    labels = torch.stack(labels, 0)\n",
    "    return g, labels\n",
    "\n",
    "# load dataset\n",
    "dataset = DglGraphPropPredDataset(name='ogbg-molhiv')\n",
    "split_idx = dataset.get_idx_split()\n",
    "# dataloader\n",
    "train_loader = GraphDataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, collate_fn=_collate_fn)\n",
    "valid_loader = GraphDataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)\n",
    "test_loader = GraphDataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, collate_fn=_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Node Property Prediction datasets in OGB\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name='ogbn-proteins')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# there is only one graph in Node Property Prediction datasets\n",
    "g, labels = dataset[0]\n",
    "# get split labels\n",
    "train_label = dataset.labels[split_idx['train']]\n",
    "valid_label = dataset.labels[split_idx['valid']]\n",
    "test_label = dataset.labels[split_idx['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Link Property Prediction datasets in OGB\n",
    "from ogb.linkproppred import DglLinkPropPredDataset\n",
    "\n",
    "dataset = DglLinkPropPredDataset(name='ogbl-ppa')\n",
    "split_edge = dataset.get_edge_split()\n",
    "\n",
    "graph = dataset[0]\n",
    "print(split_edge['train'].keys())\n",
    "print(split_edge['valid'].keys())\n",
    "print(split_edge['test'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "ds = dgl.data.CSVDataset('/path/to/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demonstration of how to use the loaded dataset. The feature names\n",
    "# may vary depending on the CSV contents.\n",
    "g = ds[0] # get the graph\n",
    "label = g.ndata['label']\n",
    "feat = g.ndata['feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyDataParser:\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        parsed = {}\n",
    "        for header in df:\n",
    "            if 'Unnamed' in header:  # Handle Unnamed column\n",
    "                print(\"Unamed column is found. Ignored...\")\n",
    "                continue\n",
    "            dt = df[header].to_numpy().squeeze()\n",
    "            if header == 'label':\n",
    "                dt = np.array([1 if e == 'positive' else 0 for e in dt])\n",
    "            parsed[header] = dt\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "dataset = dgl.data.CSVDataset('./customized_parser_dataset',\n",
    "                              ndata_parser=MyDataParser(),\n",
    "                              edata_parser=MyDataParser())\n",
    "print(dataset[0].ndata['label'])\n",
    "print(dataset[0].edata['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dglemv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
